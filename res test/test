{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"italian\")\n",
    "lemma = stemmer.stem(\"cani\")\n",
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cani'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma = lemmatizer.lemmatize(\"cani\")\n",
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetLemmatizer' object has no attribute 'lemma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f2c9e689e7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcane_lemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cane\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ita\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'WordNetLemmatizer' object has no attribute 'lemma'"
     ]
    }
   ],
   "source": [
    "cane_lemmas = lemmatizer.lemma(\"cane\", lang=\"ita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TreeTaggerError",
     "evalue": "Can't locate TreeTagger directory (and no TAGDIR specified).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTreeTaggerError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bc3f72518cbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mit_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo. Stasera mangio la pizza con le verdure.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtreetaggerwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreeTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTAGLANG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/treetaggerwrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using treetaggerwrapper.py from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;31m# Note: TreeTagger process is started later, when really needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/treetaggerwrapper.py\u001b[0m in \u001b[0;36m_set_tagger\u001b[0;34m(self, kargs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 logger.error(\"Can't locate TreeTagger directory (and \"\n\u001b[1;32m   1047\u001b[0m                              \"no TAGDIR specified).\")\n\u001b[0;32m-> 1048\u001b[0;31m                 raise TreeTaggerError(\"Can't locate TreeTagger directory (and \"\n\u001b[0m\u001b[1;32m   1049\u001b[0m                                       \"no TAGDIR specified).\")\n\u001b[1;32m   1050\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTreeTaggerError\u001b[0m: Can't locate TreeTagger directory (and no TAGDIR specified)."
     ]
    }
   ],
   "source": [
    "import treetaggerwrapper \n",
    "\n",
    "it_string = \"Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo. Stasera mangio la pizza con le verdure.\"\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG=\"it\")\n",
    "tags = tagger.tag_text(it_string)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['avra', 'avro', 'fara', 'faro', 'giu', 'perche', 'pero', 'piu', 'sara', 'saro', 'stara', 'staro'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abbastanza',\n",
       " 'acqua',\n",
       " 'adiacente',\n",
       " 'affaccia',\n",
       " 'affezionati',\n",
       " 'affollato',\n",
       " 'affrontare',\n",
       " 'agosto',\n",
       " 'aiutato',\n",
       " 'alcune',\n",
       " 'andare',\n",
       " 'andati',\n",
       " 'anno',\n",
       " 'appeso',\n",
       " 'approccio',\n",
       " 'approfittare',\n",
       " 'asciugamani',\n",
       " 'aspetto',\n",
       " 'autobus',\n",
       " 'avviso',\n",
       " 'bagno',\n",
       " 'baia',\n",
       " 'bar',\n",
       " 'basso',\n",
       " 'bel',\n",
       " 'belle',\n",
       " 'ben',\n",
       " 'bevande',\n",
       " 'bicicletta',\n",
       " 'blu',\n",
       " 'buona',\n",
       " 'buone',\n",
       " 'buono',\n",
       " \"c'erano\",\n",
       " 'calda',\n",
       " 'cambiamento',\n",
       " 'camera',\n",
       " 'camere',\n",
       " 'caratteristiche',\n",
       " 'caro',\n",
       " 'cerca',\n",
       " 'cibo',\n",
       " 'cima',\n",
       " 'cio',\n",
       " 'circolare',\n",
       " 'classe',\n",
       " 'client',\n",
       " 'clienti',\n",
       " 'colazione',\n",
       " 'collina',\n",
       " 'coloro',\n",
       " 'complessiva',\n",
       " 'completamente',\n",
       " 'confortevoli',\n",
       " 'connessione',\n",
       " 'contenuti',\n",
       " 'continentale',\n",
       " 'conto',\n",
       " 'contribuire',\n",
       " 'coraggio',\n",
       " 'cordiale',\n",
       " 'cosa',\n",
       " 'cosi',\n",
       " 'credo',\n",
       " 'cristallino',\n",
       " 'crociera',\n",
       " 'crociere',\n",
       " 'cuore',\n",
       " 'curati',\n",
       " \"dall'edificio\",\n",
       " 'deboli',\n",
       " 'decisamente',\n",
       " \"dell'hotel\",\n",
       " \"dell'isola\",\n",
       " \"dell'italia\",\n",
       " 'devo',\n",
       " 'difficile',\n",
       " 'dimensioni',\n",
       " 'dire',\n",
       " 'direi',\n",
       " 'direttamente',\n",
       " 'disponibile',\n",
       " 'doccia',\n",
       " 'dovrebbe',\n",
       " 'due',\n",
       " 'durante',\n",
       " 'eccellente',\n",
       " 'eccezionale',\n",
       " 'efficiente',\n",
       " 'elegantemente',\n",
       " 'entrambi',\n",
       " 'entrare',\n",
       " 'esigenza',\n",
       " 'esperienza',\n",
       " 'essere',\n",
       " 'facile',\n",
       " 'facilita',\n",
       " 'famiglie',\n",
       " 'fatto',\n",
       " 'fedelissimi',\n",
       " 'fiera',\n",
       " 'finale',\n",
       " 'findng',\n",
       " 'fine',\n",
       " 'finestra',\n",
       " 'firma',\n",
       " 'firmato',\n",
       " 'fisiche',\n",
       " 'fisso',\n",
       " 'fuori',\n",
       " 'gentile',\n",
       " 'gestito',\n",
       " 'ghiaia',\n",
       " 'giardini',\n",
       " 'gita',\n",
       " 'giu',\n",
       " 'giugno',\n",
       " 'gradini',\n",
       " 'grado',\n",
       " 'grande',\n",
       " 'gratuita',\n",
       " 'guarda',\n",
       " 'gusto',\n",
       " 'hotel',\n",
       " 'ideale',\n",
       " 'impegnativa',\n",
       " 'inferiore',\n",
       " 'iniziale',\n",
       " 'internet',\n",
       " 'ischia',\n",
       " \"l'architetto\",\n",
       " \"l'aumento\",\n",
       " \"l'esperienza\",\n",
       " \"l'hotel\",\n",
       " \"l'isola\",\n",
       " \"l'occhio\",\n",
       " \"l'opzione\",\n",
       " 'lasciate',\n",
       " 'leggermente',\n",
       " 'lettini',\n",
       " 'limitati',\n",
       " 'linda',\n",
       " 'luigi',\n",
       " 'luigis',\n",
       " 'lussuoso',\n",
       " 'macchina',\n",
       " 'maggior',\n",
       " 'mai',\n",
       " 'mangiato',\n",
       " 'mantenere',\n",
       " 'mare',\n",
       " 'mario',\n",
       " 'med',\n",
       " 'meno',\n",
       " 'meta',\n",
       " 'mezza',\n",
       " 'migliorare',\n",
       " 'migliori',\n",
       " 'minuti',\n",
       " 'mobilita',\n",
       " 'modo',\n",
       " 'molti',\n",
       " 'molto',\n",
       " 'montagne',\n",
       " 'motivo',\n",
       " 'navi',\n",
       " 'negative',\n",
       " 'nessuno',\n",
       " 'niente',\n",
       " 'noleggiato',\n",
       " 'notte',\n",
       " 'notti',\n",
       " 'nuotare',\n",
       " 'occupato',\n",
       " 'ogni',\n",
       " 'ore',\n",
       " 'organizzata',\n",
       " 'ormeggiate',\n",
       " 'ottenere',\n",
       " 'parere',\n",
       " 'parla',\n",
       " 'parlando',\n",
       " 'parte',\n",
       " 'pasta',\n",
       " 'pavimentazione',\n",
       " 'pena',\n",
       " 'pensione',\n",
       " 'permanente',\n",
       " 'personale',\n",
       " 'peso',\n",
       " 'piccola',\n",
       " 'piccole',\n",
       " 'piccolo',\n",
       " 'piedi',\n",
       " 'piscina',\n",
       " 'piscine',\n",
       " 'piu',\n",
       " 'piuttosto',\n",
       " 'plastica',\n",
       " 'porto',\n",
       " 'posizione',\n",
       " 'potesse',\n",
       " 'poteva',\n",
       " 'potuto',\n",
       " 'pranzo',\n",
       " 'preferito',\n",
       " 'prendere',\n",
       " 'prendete',\n",
       " 'presso',\n",
       " 'prevede',\n",
       " 'prezzo',\n",
       " 'prima',\n",
       " 'principale',\n",
       " 'privato',\n",
       " 'problema',\n",
       " 'problemi',\n",
       " 'propria',\n",
       " 'pulite',\n",
       " 'puo',\n",
       " 'quali',\n",
       " 'quando',\n",
       " 'raggiungere',\n",
       " 'rara',\n",
       " 'realta',\n",
       " 'recensioni',\n",
       " 'reception',\n",
       " 'relax',\n",
       " 'riposo',\n",
       " 'ristorante',\n",
       " 'ritirate',\n",
       " 'roberta',\n",
       " 'sacco',\n",
       " 'sapientemente',\n",
       " 'scarpe',\n",
       " 'scogliera',\n",
       " 'scolpita',\n",
       " 'scommettitori',\n",
       " 'scorciatoia',\n",
       " 'seguendo',\n",
       " 'sera',\n",
       " 'servito',\n",
       " 'servizio',\n",
       " 'set',\n",
       " 'settimana',\n",
       " 'sicuro',\n",
       " 'sinistra',\n",
       " 'soggiornato',\n",
       " 'solo',\n",
       " 'somma',\n",
       " 'sorta',\n",
       " 'sostituzione',\n",
       " 'sottofondo',\n",
       " 'sottostante',\n",
       " 'spappola',\n",
       " 'spazio',\n",
       " 'specialita',\n",
       " 'sperimentato',\n",
       " 'spiaggia',\n",
       " 'splendida',\n",
       " 'squadra',\n",
       " 'stata',\n",
       " 'state',\n",
       " 'stati',\n",
       " 'stessa',\n",
       " 'stima',\n",
       " 'strada',\n",
       " 'stravagante',\n",
       " 'struttura',\n",
       " \"sull'isola\",\n",
       " 'sun',\n",
       " 'superiore',\n",
       " 'tanti',\n",
       " 'tanto',\n",
       " 'taxi',\n",
       " 'team',\n",
       " 'tempi',\n",
       " 'terminal',\n",
       " 'terraces',\n",
       " 'terrazza',\n",
       " 'torna',\n",
       " 'totale',\n",
       " 'tranquilla',\n",
       " 'trasporto',\n",
       " 'trattati',\n",
       " 'tratto',\n",
       " 'treno',\n",
       " 'trova',\n",
       " 'trovare',\n",
       " 'trovato',\n",
       " 'tuttavia',\n",
       " 'tutte',\n",
       " \"un'esperienza\",\n",
       " 'uscire',\n",
       " 'utilizzato',\n",
       " 'vacanze',\n",
       " 'vale',\n",
       " 'vario',\n",
       " 'vedere',\n",
       " 'vedute',\n",
       " 'venezia',\n",
       " 'verifichi',\n",
       " 'verso',\n",
       " 'viene',\n",
       " 'vigile',\n",
       " 'visita',\n",
       " 'visitare',\n",
       " 'vista',\n",
       " 'visualizzata',\n",
       " 'wi-fi',\n",
       " 'zag',\n",
       " 'zig']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('./data/development.csv', sep = ',')\n",
    "x = df_train['text'].loc[0:1]\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def hasVowels(inputString):\n",
    "    vowels = {\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"}\n",
    "    return any(char in vowels for char in inputString)\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, document): \n",
    "        lemmas = []\n",
    "        stemmer = SnowballStemmer(\"italian\")\n",
    "        for t in word_tokenize(document):\n",
    "            valid = True\n",
    "            if (t in string.punctuation or hasNumbers(t) or not hasVowels(t)):\n",
    "                valid = False\n",
    "            #lemma = stemmer.stem(t)\n",
    "            #for char in lemma:\n",
    "            #    if char in string.punctuation:\n",
    "            #        lemma = lemma.replace(char, ' ')\n",
    "            lemma = t.strip()\n",
    "            # remove tokens with only punctuation chars and digits\n",
    "            #re_digit = re.compile(\"[0-9]\") # regular expression to filter digit 􏰀tokens not re_digit.match(lemma)\n",
    "            if len(lemma) > 2 and len(lemma) < 16 and valid:\n",
    "                lemmas.append(lemma) \n",
    "        return lemmas\n",
    "    \n",
    "stopwords = get_stop_words('italian') #+(list_sw)\n",
    "tokenizer = LemmaTokenizer()\n",
    "vectorizer = TfidfVectorizer(input = \"content\",tokenizer = tokenizer,stop_words=stopwords,encoding='utf-8',strip_accents = 'unicode',lowercase=True)\n",
    "X_tfidf = vectorizer.fit_transform(x)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f52ed9c5d7c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'안녕하세요.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "import copy\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "translator.translate('안녕하세요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Non\\tADV\\tnon', '+\\tSYM\\t+']\n",
      "Non\tADV\tnon\n",
      "3\n",
      "['Non', 'ADV', 'non']\n",
      "+\tSYM\t+\n",
      "3\n",
      "['+', 'SYM', '+']\n"
     ]
    }
   ],
   "source": [
    "import treetaggerwrapper\n",
    "import re\n",
    "\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='it')\n",
    "tags = tagger.tag_text(\"Non\\t+\")\n",
    "print(tags)\n",
    "\n",
    "for tag in tags:\n",
    "    print(tag)\n",
    "    tagged = re.split(r'\\t+', tag)\n",
    "    print(len(tagged))\n",
    "    #lemma = tagged[2]\n",
    "    #pos = tagged[1]\n",
    "    #print(lemma)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string  \n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad',\n",
       " 'al',\n",
       " 'allo',\n",
       " 'ai',\n",
       " 'agli',\n",
       " 'all',\n",
       " 'agl',\n",
       " 'alla',\n",
       " 'alle',\n",
       " 'con',\n",
       " 'col',\n",
       " 'coi',\n",
       " 'da',\n",
       " 'dal',\n",
       " 'dallo',\n",
       " 'dai',\n",
       " 'dagli',\n",
       " 'dall',\n",
       " 'dagl',\n",
       " 'dalla',\n",
       " 'dalle',\n",
       " 'di',\n",
       " 'del',\n",
       " 'dello',\n",
       " 'dei',\n",
       " 'degli',\n",
       " 'dell',\n",
       " 'degl',\n",
       " 'della',\n",
       " 'delle',\n",
       " 'in',\n",
       " 'nel',\n",
       " 'nello',\n",
       " 'nei',\n",
       " 'negli',\n",
       " 'nell',\n",
       " 'negl',\n",
       " 'nella',\n",
       " 'nelle',\n",
       " 'su',\n",
       " 'sul',\n",
       " 'sullo',\n",
       " 'sui',\n",
       " 'sugli',\n",
       " 'sull',\n",
       " 'sugl',\n",
       " 'sulla',\n",
       " 'sulle',\n",
       " 'per',\n",
       " 'tra',\n",
       " 'contro',\n",
       " 'io',\n",
       " 'tu',\n",
       " 'lui',\n",
       " 'lei',\n",
       " 'noi',\n",
       " 'voi',\n",
       " 'loro',\n",
       " 'mio',\n",
       " 'mia',\n",
       " 'miei',\n",
       " 'mie',\n",
       " 'tuo',\n",
       " 'tua',\n",
       " 'tuoi',\n",
       " 'tue',\n",
       " 'suo',\n",
       " 'sua',\n",
       " 'suoi',\n",
       " 'sue',\n",
       " 'nostro',\n",
       " 'nostra',\n",
       " 'nostri',\n",
       " 'nostre',\n",
       " 'vostro',\n",
       " 'vostra',\n",
       " 'vostri',\n",
       " 'vostre',\n",
       " 'mi',\n",
       " 'ti',\n",
       " 'ci',\n",
       " 'vi',\n",
       " 'lo',\n",
       " 'la',\n",
       " 'li',\n",
       " 'le',\n",
       " 'gli',\n",
       " 'ne',\n",
       " 'il',\n",
       " 'un',\n",
       " 'uno',\n",
       " 'una',\n",
       " 'ma',\n",
       " 'ed',\n",
       " 'se',\n",
       " 'perché',\n",
       " 'anche',\n",
       " 'come',\n",
       " 'dov',\n",
       " 'dove',\n",
       " 'che',\n",
       " 'chi',\n",
       " 'cui',\n",
       " 'non',\n",
       " 'più',\n",
       " 'quale',\n",
       " 'quanto',\n",
       " 'quanti',\n",
       " 'quanta',\n",
       " 'quante',\n",
       " 'quello',\n",
       " 'quelli',\n",
       " 'quella',\n",
       " 'quelle',\n",
       " 'questo',\n",
       " 'questi',\n",
       " 'questa',\n",
       " 'queste',\n",
       " 'si',\n",
       " 'tutto',\n",
       " 'tutti',\n",
       " 'a',\n",
       " 'c',\n",
       " 'e',\n",
       " 'i',\n",
       " 'l',\n",
       " 'o',\n",
       " 'ho',\n",
       " 'hai',\n",
       " 'ha',\n",
       " 'abbiamo',\n",
       " 'avete',\n",
       " 'hanno',\n",
       " 'abbia',\n",
       " 'abbiate',\n",
       " 'abbiano',\n",
       " 'avrò',\n",
       " 'avrai',\n",
       " 'avrà',\n",
       " 'avremo',\n",
       " 'avrete',\n",
       " 'avranno',\n",
       " 'avrei',\n",
       " 'avresti',\n",
       " 'avrebbe',\n",
       " 'avremmo',\n",
       " 'avreste',\n",
       " 'avrebbero',\n",
       " 'avevo',\n",
       " 'avevi',\n",
       " 'aveva',\n",
       " 'avevamo',\n",
       " 'avevate',\n",
       " 'avevano',\n",
       " 'ebbi',\n",
       " 'avesti',\n",
       " 'ebbe',\n",
       " 'avemmo',\n",
       " 'aveste',\n",
       " 'ebbero',\n",
       " 'avessi',\n",
       " 'avesse',\n",
       " 'avessimo',\n",
       " 'avessero',\n",
       " 'avendo',\n",
       " 'avuto',\n",
       " 'avuta',\n",
       " 'avuti',\n",
       " 'avute',\n",
       " 'sono',\n",
       " 'sei',\n",
       " 'è',\n",
       " 'siamo',\n",
       " 'siete',\n",
       " 'sia',\n",
       " 'siate',\n",
       " 'siano',\n",
       " 'sarò',\n",
       " 'sarai',\n",
       " 'sarà',\n",
       " 'saremo',\n",
       " 'sarete',\n",
       " 'saranno',\n",
       " 'sarei',\n",
       " 'saresti',\n",
       " 'sarebbe',\n",
       " 'saremmo',\n",
       " 'sareste',\n",
       " 'sarebbero',\n",
       " 'ero',\n",
       " 'eri',\n",
       " 'era',\n",
       " 'eravamo',\n",
       " 'eravate',\n",
       " 'erano',\n",
       " 'fui',\n",
       " 'fosti',\n",
       " 'fu',\n",
       " 'fummo',\n",
       " 'foste',\n",
       " 'furono',\n",
       " 'fossi',\n",
       " 'fosse',\n",
       " 'fossimo',\n",
       " 'fossero',\n",
       " 'essendo',\n",
       " 'faccio',\n",
       " 'fai',\n",
       " 'facciamo',\n",
       " 'fanno',\n",
       " 'faccia',\n",
       " 'facciate',\n",
       " 'facciano',\n",
       " 'farò',\n",
       " 'farai',\n",
       " 'farà',\n",
       " 'faremo',\n",
       " 'farete',\n",
       " 'faranno',\n",
       " 'farei',\n",
       " 'faresti',\n",
       " 'farebbe',\n",
       " 'faremmo',\n",
       " 'fareste',\n",
       " 'farebbero',\n",
       " 'facevo',\n",
       " 'facevi',\n",
       " 'faceva',\n",
       " 'facevamo',\n",
       " 'facevate',\n",
       " 'facevano',\n",
       " 'feci',\n",
       " 'facesti',\n",
       " 'fece',\n",
       " 'facemmo',\n",
       " 'faceste',\n",
       " 'fecero',\n",
       " 'facessi',\n",
       " 'facesse',\n",
       " 'facessimo',\n",
       " 'facessero',\n",
       " 'facendo',\n",
       " 'sto',\n",
       " 'stai',\n",
       " 'sta',\n",
       " 'stiamo',\n",
       " 'stanno',\n",
       " 'stia',\n",
       " 'stiate',\n",
       " 'stiano',\n",
       " 'starò',\n",
       " 'starai',\n",
       " 'starà',\n",
       " 'staremo',\n",
       " 'starete',\n",
       " 'staranno',\n",
       " 'starei',\n",
       " 'staresti',\n",
       " 'starebbe',\n",
       " 'staremmo',\n",
       " 'stareste',\n",
       " 'starebbero',\n",
       " 'stavo',\n",
       " 'stavi',\n",
       " 'stava',\n",
       " 'stavamo',\n",
       " 'stavate',\n",
       " 'stavano',\n",
       " 'stetti',\n",
       " 'stesti',\n",
       " 'stette',\n",
       " 'stemmo',\n",
       " 'steste',\n",
       " 'stettero',\n",
       " 'stessi',\n",
       " 'stesse',\n",
       " 'stessimo',\n",
       " 'stessero',\n",
       " 'stando']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "sw.words('italian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Questa', 'PRON', 'Questa'), ('è', 'VERB', 'essere'), ('una', 'DET', 'una'), ('frasetta', 'NOUN', 'frasetta'), ('.', 'PUNCT', '.')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "doc = nlp(\"Questa è una frasetta.\")\n",
    "print([(w.text, w.pos_, w.lemma_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
